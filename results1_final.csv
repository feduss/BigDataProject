Decision_Tree MLLib: 18 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9234,0.0293,0.0987,0.0214,0.9013,0.0214,0.0766,0.0293,0.0883,0.0187,0.9117,0.0187,0.8493,0.4918
2,0.9182,0.028,0.0993,0.0258,0.9007,0.0258,0.0818,0.028,0.0911,0.0225,0.9087,0.0224,0.621,0.0708
3,0.929,0.0353,0.0915,0.0195,0.9085,0.0195,0.071,0.0353,0.0822,0.0169,0.9178,0.0165,0.6847,0.0984
4,0.9087,0.0287,0.0996,0.0209,0.9004,0.0209,0.0913,0.0287,0.0957,0.019,0.9044,0.019,0.6083,0.0643
5,0.9193,0.0301,0.1018,0.0209,0.8982,0.0209,0.0808,0.0301,0.0918,0.021,0.9081,0.0209,0.641,0.0737
6,0.9101,0.0354,0.0963,0.0198,0.9037,0.0198,0.0899,0.0354,0.0934,0.02,0.9068,0.0197,0.661,0.0944
7,0.8995,0.0303,0.0964,0.0209,0.9036,0.0209,0.1005,0.0303,0.0987,0.0182,0.9015,0.0182,0.6135,0.0657
8,0.9088,0.0293,0.1024,0.0209,0.8976,0.0209,0.0912,0.0293,0.0969,0.022,0.9031,0.0218,0.657,0.0736
9,0.8996,0.0305,0.0943,0.0213,0.9057,0.0213,0.1004,0.0305,0.0977,0.0174,0.9025,0.0171,0.6671,0.0614
10,0.9312,0.0264,0.0977,0.0278,0.9023,0.0278,0.0688,0.0264,0.0846,0.0154,0.9154,0.0156,0.6008,0.0936
11,0.9245,0.0286,0.0982,0.0254,0.9018,0.0254,0.0755,0.0286,0.088,0.0116,0.9121,0.0113,0.6359,0.1049
12,0.924,0.0254,0.1049,0.0181,0.8951,0.0181,0.076,0.0254,0.0915,0.0117,0.9084,0.012,0.6326,0.0897
13,0.9207,0.0248,0.0961,0.0248,0.9039,0.0248,0.0793,0.0248,0.0884,0.014,0.9117,0.0142,0.6328,0.0723
14,0.9133,0.0187,0.0954,0.0249,0.9046,0.0249,0.0867,0.0187,0.0915,0.0112,0.9086,0.0114,0.6568,0.0621
15,0.9135,0.0273,0.0977,0.0252,0.9023,0.0252,0.0865,0.0273,0.0929,0.0111,0.9074,0.0113,0.6895,0.0827
16,0.92,0.0294,0.0949,0.0253,0.9052,0.0253,0.08,0.0294,0.0883,0.0136,0.9119,0.0137,0.6581,0.068
17,0.907,0.018,0.0955,0.0226,0.9045,0.0226,0.093,0.018,0.0945,0.0097,0.9056,0.0098,0.636,0.0784
18,0.9031,0.0246,0.0935,0.0192,0.9065,0.0192,0.0969,0.0246,0.0953,0.0148,0.9049,0.0148,0.7642,0.1248
Best version of Decision_Tree MLLib is test n°2
-
Mean and St_Dev of the whole Decision_Tree MLLib classifier metrics are as follows:
Sensitivity: Mean = 0.9152; St_Dev = 0.0096
Fallout: Mean = 0.0975; St_Dev = 0.0033
Specificity: Mean = 0.9025; St_Dev = 0.0033
Miss_Rate: Mean = 0.0848; St_Dev = 0.0096
Test_Error: Mean = 0.0917; St_Dev = 0.0045
AUC: Mean = 0.9084; St_Dev = 0.0044
Exec_Time: Mean = 0.6616; St_Dev = 0.0599
#############################
Random_Forest MLLib: 36 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9801,0.011,0.1099,0.0213,0.8901,0.0213,0.0199,0.011,0.0704,0.0134,0.9289,0.0127,0.9151,0.1085
2,0.9765,0.0106,0.1108,0.0209,0.8892,0.0209,0.0235,0.0106,0.0724,0.013,0.9269,0.0119,1.179,0.1293
3,0.9735,0.0129,0.1122,0.0231,0.8878,0.0231,0.0265,0.0129,0.0746,0.0147,0.9246,0.014,1.0362,0.122
4,0.9758,0.0112,0.1088,0.0225,0.8912,0.0225,0.0242,0.0112,0.0715,0.0134,0.9279,0.0126,1.3677,0.1642
5,0.9784,0.0116,0.1101,0.0247,0.8899,0.0247,0.0216,0.0116,0.0712,0.0158,0.9281,0.0152,1.1714,0.0702
6,0.9774,0.0108,0.1092,0.0241,0.8908,0.0241,0.0226,0.0108,0.0712,0.0141,0.9281,0.0133,1.7217,0.1271
7,0.9737,0.013,0.1078,0.0215,0.8922,0.0215,0.0263,0.013,0.0717,0.0138,0.9276,0.0129,0.95,0.0541
8,0.9772,0.0082,0.1048,0.0232,0.8952,0.0232,0.0227,0.0082,0.0684,0.0134,0.931,0.0126,1.3677,0.1154
9,0.9736,0.0082,0.1062,0.0262,0.8938,0.0262,0.0264,0.0082,0.0709,0.0157,0.9285,0.015,1.1074,0.0864
10,0.9788,0.0097,0.1069,0.0227,0.8931,0.0227,0.0212,0.0097,0.0691,0.0145,0.9301,0.0134,1.6173,0.0688
11,0.976,0.0112,0.1022,0.023,0.8978,0.023,0.024,0.0112,0.0674,0.0142,0.932,0.0131,1.3145,0.047
12,0.973,0.011,0.1041,0.0234,0.8959,0.0234,0.027,0.011,0.0698,0.0134,0.9296,0.0126,2.118,0.0881
13,0.9746,0.0131,0.1005,0.0268,0.8995,0.0268,0.0254,0.0131,0.067,0.0164,0.9326,0.0159,1.0771,0.0855
14,0.974,0.0124,0.1017,0.0238,0.8983,0.0238,0.026,0.0124,0.068,0.0135,0.9315,0.0126,1.5531,0.0955
15,0.9739,0.007,0.0985,0.0202,0.9015,0.0202,0.0261,0.007,0.066,0.0103,0.9334,0.0093,1.2172,0.0856
16,0.9752,0.0088,0.1,0.0262,0.9,0.0262,0.0248,0.0088,0.0664,0.0158,0.9331,0.015,1.8384,0.0899
17,0.9715,0.0105,0.0998,0.0257,0.9002,0.0257,0.0285,0.0105,0.0678,0.0148,0.9318,0.0143,1.5059,0.1057
18,0.9746,0.0113,0.1001,0.0241,0.8999,0.0241,0.0254,0.0113,0.0667,0.0152,0.9328,0.0144,2.3899,0.136
19,0.9793,0.0114,0.1127,0.0203,0.8873,0.0203,0.0207,0.0114,0.0724,0.0115,0.9268,0.0106,0.7993,0.0457
20,0.9801,0.0103,0.112,0.0221,0.888,0.0221,0.0199,0.0103,0.0718,0.0118,0.9275,0.0111,1.1568,0.0789
21,0.9786,0.0117,0.1128,0.024,0.8872,0.024,0.0214,0.0117,0.0729,0.0149,0.9264,0.0142,0.9172,0.0871
22,0.9799,0.008,0.1136,0.0256,0.8864,0.0256,0.0201,0.008,0.0728,0.016,0.9265,0.0151,1.3239,0.1305
23,0.9771,0.009,0.1097,0.0237,0.8903,0.0237,0.0229,0.009,0.0715,0.0143,0.9279,0.0134,1.1788,0.0948
24,0.9791,0.0047,0.1132,0.0229,0.8868,0.0229,0.0209,0.0047,0.0729,0.0139,0.9263,0.0132,1.7471,0.1532
25,0.9757,0.0127,0.1103,0.0218,0.8897,0.0218,0.0243,0.0127,0.0724,0.0133,0.9269,0.0125,0.9675,0.1048
26,0.9778,0.007,0.1096,0.0208,0.8904,0.0208,0.0222,0.007,0.0711,0.0113,0.9282,0.0105,1.3111,0.0814
27,0.9773,0.0082,0.1047,0.024,0.8953,0.024,0.0227,0.0082,0.0683,0.014,0.9311,0.013,1.0336,0.0724
28,0.9772,0.011,0.1065,0.023,0.8935,0.023,0.0228,0.011,0.0694,0.0146,0.9299,0.0137,1.5868,0.144
29,0.9766,0.0075,0.1055,0.0229,0.8945,0.0229,0.0234,0.0075,0.0691,0.013,0.9302,0.012,1.2779,0.0719
30,0.9758,0.0094,0.1071,0.0219,0.8929,0.0219,0.0242,0.0094,0.0705,0.0128,0.9288,0.012,2.1198,0.125
31,0.978,0.0111,0.1024,0.0245,0.8976,0.0246,0.022,0.0111,0.0666,0.0141,0.9329,0.0134,1.0711,0.085
32,0.9735,0.0101,0.1071,0.024,0.8929,0.024,0.0265,0.0101,0.0714,0.0139,0.928,0.013,1.5361,0.0843
33,0.9758,0.0066,0.1011,0.0255,0.8989,0.0255,0.0242,0.0066,0.0668,0.0149,0.9327,0.014,1.2125,0.0909
34,0.9759,0.0076,0.1038,0.0217,0.8962,0.0217,0.0241,0.0076,0.0684,0.0121,0.931,0.0112,1.8386,0.1195
35,0.9715,0.0091,0.1015,0.0226,0.8985,0.0226,0.0285,0.0091,0.0688,0.0123,0.9307,0.0115,1.4978,0.0817
36,0.9737,0.0063,0.1018,0.0233,0.8982,0.0233,0.0264,0.0063,0.0681,0.0127,0.9314,0.0118,2.4501,0.1862
Best version of Random_Forest MLLib is test n°14
-
Mean and St_Dev of the whole Random_Forest MLLib classifier metrics are as follows:
Sensitivity: Mean = 0.9761; St_Dev = 0.0024
Fallout: Mean = 0.1064; St_Dev = 0.0045
Specificity: Mean = 0.8936; St_Dev = 0.0045
Miss_Rate: Mean = 0.0239; St_Dev = 0.0024
Test_Error: Mean = 0.0699; St_Dev = 0.0023
AUC: Mean = 0.9295; St_Dev = 0.0024
Exec_Time: Mean = 1.402; St_Dev = 0.413
#############################
Random_Forest Sklearn: 36 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9737,0.0113,0.1106,0.0248,0.8894,0.0248,0.0263,0.0113,0.0736,0.0149,0.9257,0.0142,1.1308,0.1964
2,0.972,0.0071,0.108,0.0227,0.892,0.0227,0.028,0.0071,0.0725,0.0127,0.9268,0.012,1.2066,0.1341
3,0.9727,0.0128,0.1105,0.0287,0.8895,0.0287,0.0273,0.0128,0.0739,0.0171,0.9255,0.0165,1.2814,0.1264
4,0.9729,0.0125,0.1074,0.0222,0.8926,0.0222,0.0271,0.0125,0.0718,0.0128,0.9276,0.0119,1.4568,0.081
5,0.9759,0.01,0.1072,0.0223,0.8928,0.0223,0.0241,0.01,0.0705,0.0131,0.9288,0.0122,1.4084,0.139
6,0.9757,0.0101,0.1125,0.0235,0.8875,0.0235,0.0243,0.0101,0.0738,0.0142,0.9254,0.0132,1.3894,0.069
7,0.9729,0.0079,0.1041,0.0248,0.8959,0.0248,0.0271,0.0079,0.0699,0.0138,0.9296,0.0131,1.318,0.1111
8,0.9724,0.0103,0.0998,0.0234,0.9002,0.0234,0.0276,0.0103,0.0674,0.0131,0.9321,0.0122,1.2862,0.1353
9,0.9728,0.0085,0.1052,0.0226,0.8948,0.0226,0.0272,0.0085,0.0705,0.0119,0.929,0.0113,1.3274,0.1384
10,0.9695,0.0122,0.1027,0.0225,0.8973,0.0225,0.0305,0.0122,0.0704,0.0122,0.9291,0.0113,1.4717,0.0462
11,0.97,0.0097,0.1044,0.0251,0.8956,0.0251,0.03,0.0097,0.0712,0.0142,0.9282,0.0134,1.4218,0.143
12,0.9758,0.0064,0.105,0.0229,0.895,0.0229,0.0243,0.0064,0.0691,0.014,0.9302,0.013,1.4648,0.0862
13,0.9698,0.0127,0.0961,0.0264,0.9039,0.0264,0.0302,0.0127,0.0663,0.0163,0.9332,0.0156,1.3349,0.0635
14,0.969,0.0142,0.0977,0.0271,0.9023,0.0271,0.031,0.0142,0.0677,0.0163,0.9319,0.0157,1.3263,0.1222
15,0.9709,0.0092,0.0993,0.0233,0.9006,0.0233,0.0291,0.0092,0.0678,0.0132,0.9316,0.0123,1.3033,0.0575
16,0.9689,0.0104,0.0973,0.0234,0.9027,0.0234,0.0311,0.0104,0.0674,0.0128,0.9321,0.012,1.5071,0.1079
17,0.9682,0.013,0.0996,0.0227,0.9004,0.0227,0.0318,0.013,0.0691,0.0129,0.9304,0.0121,1.4389,0.093
18,0.9717,0.0106,0.1009,0.0222,0.8991,0.0222,0.0283,0.0106,0.0684,0.0135,0.931,0.0127,1.4567,0.1121
19,0.9758,0.0131,0.1088,0.0228,0.8912,0.0228,0.0242,0.0131,0.0716,0.014,0.9277,0.0132,1.3377,0.0888
20,0.9756,0.0089,0.1109,0.0248,0.8891,0.0248,0.0244,0.0089,0.0729,0.015,0.9264,0.0143,1.3779,0.133
21,0.9794,0.0091,0.1143,0.023,0.8857,0.023,0.0206,0.0091,0.0736,0.0145,0.9256,0.0134,1.3315,0.0969
22,0.9764,0.0068,0.1097,0.0236,0.8903,0.0236,0.0236,0.0068,0.0718,0.0141,0.9275,0.0131,1.5382,0.0815
23,0.977,0.0042,0.1113,0.024,0.8887,0.024,0.023,0.0042,0.0726,0.0137,0.9267,0.0128,1.5145,0.106
24,0.9762,0.0058,0.1134,0.0231,0.8866,0.0231,0.0238,0.0058,0.0742,0.013,0.9251,0.0122,1.4904,0.1288
25,0.9735,0.0065,0.1051,0.0246,0.8949,0.0246,0.0265,0.0065,0.0702,0.014,0.9293,0.0131,1.3533,0.0776
26,0.9735,0.0088,0.1056,0.0253,0.8944,0.0253,0.0265,0.0088,0.0705,0.0147,0.9289,0.014,1.3368,0.0927
27,0.9779,0.0069,0.1079,0.0241,0.8921,0.0241,0.0221,0.0069,0.0701,0.014,0.9293,0.013,1.3648,0.1169
28,0.9751,0.0071,0.1045,0.025,0.8955,0.025,0.0249,0.0071,0.0691,0.0142,0.9303,0.0132,1.585,0.0944
29,0.9738,0.0098,0.1013,0.0245,0.8987,0.0245,0.0261,0.0098,0.0678,0.0137,0.9317,0.0129,1.5894,0.0973
30,0.975,0.0059,0.1088,0.0239,0.8912,0.0239,0.025,0.0059,0.0719,0.0128,0.9274,0.0121,1.5813,0.0886
31,0.9731,0.0105,0.1008,0.0239,0.8992,0.0239,0.0269,0.0105,0.0677,0.0136,0.9318,0.0128,1.3971,0.1301
32,0.9737,0.0074,0.1002,0.0213,0.8998,0.0213,0.0263,0.0074,0.0671,0.0121,0.9323,0.0115,1.4136,0.1188
33,0.974,0.0105,0.1013,0.024,0.8987,0.024,0.026,0.0105,0.0678,0.0146,0.9316,0.0136,1.3435,0.1041
34,0.9744,0.0081,0.1001,0.0263,0.8999,0.0263,0.0256,0.0081,0.0668,0.015,0.9327,0.0145,1.5228,0.0856
35,0.9746,0.0086,0.0985,0.0239,0.9015,0.0239,0.0254,0.0086,0.0658,0.0134,0.9337,0.0125,1.5803,0.1286
36,0.9736,0.0076,0.1029,0.0242,0.8971,0.0242,0.0264,0.0076,0.0688,0.0141,0.9306,0.0132,1.553,0.104
Best version of Random_Forest Sklearn is test n°34
-
Mean and St_Dev of the whole Random_Forest Sklearn classifier metrics are as follows:
Sensitivity: Mean = 0.9735; St_Dev = 0.0026
Fallout: Mean = 0.1048; St_Dev = 0.005
Specificity: Mean = 0.8952; St_Dev = 0.005
Miss_Rate: Mean = 0.0265; St_Dev = 0.0026
Test_Error: Mean = 0.07; St_Dev = 0.0024
AUC: Mean = 0.9294; St_Dev = 0.0025
Exec_Time: Mean = 1.4095; St_Dev = 0.1106
#############################
Gradient Boosted Tree: 36 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9482,0.0224,0.0804,0.0229,0.9196,0.0229,0.0518,0.0224,0.067,0.0163,0.9328,0.0161,7.0008,0.1917
2,0.9486,0.0213,0.0769,0.0266,0.9231,0.0266,0.0513,0.0213,0.0651,0.0169,0.9347,0.0166,16.8888,0.5338
3,0.9422,0.0153,0.0829,0.0268,0.9171,0.0268,0.0578,0.0153,0.0712,0.0161,0.9287,0.0162,7.3694,0.1588
4,0.941,0.0195,0.082,0.0239,0.918,0.0239,0.059,0.0195,0.0711,0.0169,0.9288,0.0168,17.764,0.6616
5,0.9435,0.0269,0.0875,0.0223,0.9125,0.0223,0.0565,0.0269,0.0732,0.017,0.9265,0.0167,8.6762,0.2484
6,0.9412,0.0225,0.086,0.0231,0.914,0.0231,0.0588,0.0225,0.0734,0.0161,0.9264,0.0159,19.7729,1.0063
7,0.9242,0.0253,0.0953,0.0194,0.9048,0.0194,0.0758,0.0253,0.0861,0.0162,0.9139,0.0162,12.0346,0.6924
8,0.9265,0.0244,0.0938,0.0207,0.9062,0.0207,0.0735,0.0244,0.0842,0.017,0.9158,0.0171,31.8405,1.271
9,0.9189,0.0244,0.0947,0.024,0.9053,0.024,0.0811,0.0244,0.0882,0.0206,0.9117,0.0207,12.134,0.5862
10,0.9203,0.0231,0.0927,0.0267,0.9073,0.0267,0.0797,0.0231,0.0866,0.0204,0.9134,0.0205,31.3778,1.954
11,0.9215,0.0342,0.09,0.0192,0.91,0.0192,0.0785,0.0342,0.0848,0.0173,0.9154,0.0169,14.0194,0.5128
12,0.925,0.0339,0.0868,0.0186,0.9132,0.0186,0.075,0.0339,0.0813,0.0187,0.9189,0.0182,34.7578,1.292
13,0.9361,0.0211,0.0919,0.0223,0.9081,0.0223,0.0639,0.0211,0.0789,0.0135,0.9209,0.0136,6.9654,0.1439
14,0.9321,0.0229,0.091,0.0226,0.909,0.0226,0.0679,0.0229,0.0803,0.0154,0.9196,0.0153,18.4006,1.7069
15,0.9361,0.023,0.0928,0.0259,0.9072,0.0259,0.064,0.023,0.0795,0.0186,0.9202,0.0183,8.4667,0.3554
16,0.9336,0.0265,0.0925,0.0258,0.9075,0.0258,0.0664,0.0265,0.0805,0.0205,0.9192,0.0202,17.5078,0.4372
17,0.9324,0.0265,0.0911,0.0227,0.9089,0.0227,0.0676,0.0265,0.0805,0.0153,0.9193,0.0151,8.4481,0.3027
18,0.93,0.0254,0.0901,0.0233,0.9099,0.0233,0.07,0.0254,0.0811,0.0143,0.9188,0.014,19.2204,0.1898
19,0.9179,0.0305,0.097,0.0212,0.903,0.0212,0.0821,0.0305,0.09,0.019,0.91,0.0189,11.7985,0.2481
20,0.9179,0.0305,0.097,0.0212,0.903,0.0212,0.0821,0.0305,0.09,0.019,0.91,0.0189,31.5661,0.541
21,0.916,0.0286,0.1014,0.0247,0.8986,0.0247,0.084,0.0286,0.0931,0.0228,0.9068,0.0228,12.2175,0.2525
22,0.9166,0.0282,0.1013,0.0246,0.8987,0.0246,0.0834,0.0282,0.0928,0.0223,0.9071,0.0223,30.9536,1.5864
23,0.9199,0.0337,0.09,0.0199,0.91,0.0199,0.0801,0.0337,0.0856,0.0182,0.9146,0.0176,13.5271,0.4251
24,0.9199,0.0337,0.09,0.0199,0.91,0.0199,0.0801,0.0337,0.0856,0.0182,0.9146,0.0176,33.6339,0.9881
25,0.9337,0.0308,0.0994,0.0222,0.9006,0.0222,0.0663,0.0308,0.0844,0.0166,0.9153,0.0166,6.78,0.1255
26,0.9336,0.031,0.0999,0.0232,0.9001,0.0232,0.0664,0.031,0.0848,0.0174,0.9149,0.0175,16.3839,0.4638
27,0.9373,0.0271,0.098,0.0226,0.902,0.0226,0.0627,0.0271,0.0818,0.0195,0.9179,0.0194,7.373,0.2186
28,0.9345,0.026,0.0977,0.0224,0.9023,0.0224,0.0655,0.026,0.0828,0.0192,0.9168,0.0191,17.3671,0.3186
29,0.9303,0.0218,0.0973,0.0227,0.9027,0.0227,0.0697,0.0218,0.0848,0.0125,0.9149,0.0126,8.5425,0.3027
30,0.9302,0.025,0.0962,0.0227,0.9038,0.0227,0.0698,0.025,0.0844,0.0125,0.9153,0.0124,19.3579,0.5369
31,0.9155,0.0296,0.1001,0.0211,0.8999,0.0211,0.0844,0.0296,0.0928,0.0176,0.9073,0.0176,9.8273,0.2907
32,0.9141,0.0329,0.1026,0.0188,0.8974,0.0188,0.0859,0.0329,0.0948,0.0182,0.9052,0.0181,24.3818,1.0769
33,0.9116,0.0239,0.1011,0.019,0.8989,0.019,0.0884,0.0239,0.0951,0.0182,0.9047,0.018,10.607,0.5744
34,0.9056,0.0213,0.1004,0.0195,0.8996,0.0195,0.0944,0.0213,0.0977,0.0142,0.9023,0.0142,26.3079,1.5599
35,0.9097,0.0383,0.0915,0.0182,0.9085,0.0182,0.0903,0.0383,0.0916,0.0169,0.9087,0.0164,12.2704,0.6373
36,0.9045,0.0348,0.091,0.019,0.909,0.019,0.0955,0.0348,0.0936,0.017,0.9068,0.0165,29.1892,1.778
Best version of Gradient Boosted Tree is test n°1
-
Mean and St_Dev of the whole Gradient Boosted Tree classifier metrics are as follows:
Sensitivity: Mean = 0.927; St_Dev = 0.0117
Fallout: Mean = 0.0931; St_Dev = 0.0064
Specificity: Mean = 0.9069; St_Dev = 0.0064
Miss_Rate: Mean = 0.073; St_Dev = 0.0117
Test_Error: Mean = 0.0839; St_Dev = 0.008
AUC: Mean = 0.9161; St_Dev = 0.0079
Exec_Time: Mean = 17.0758; St_Dev = 8.861
#############################
Logistic Regression: 54 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,1.1098,0.2904
2,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,1.018,0.0537
3,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.05,0.0952
4,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,1.0049,0.0703
5,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,1.0046,0.0578
6,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.012,0.0478
7,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9978,0.0788
8,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,1.0036,0.0655
9,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,0.9934,0.0382
10,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9901,0.0802
11,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,1.0031,0.0739
12,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,0.9865,0.0396
13,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9615,0.0713
14,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.9928,0.059
15,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0069,0.0756
16,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9929,0.0441
17,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,1.0158,0.0661
18,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,0.9793,0.0517
19,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,1.0477,0.1296
20,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,1.0143,0.0613
21,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.007,0.0657
22,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9908,0.0736
23,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.9808,0.0574
24,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0185,0.0824
25,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9994,0.0972
26,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.9693,0.0397
27,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0011,0.0492
28,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,1.0027,0.0497
29,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.9919,0.0536
30,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0191,0.0949
31,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9861,0.087
32,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.992,0.0586
33,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0183,0.092
34,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,1.015,0.0628
35,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.9967,0.0603
36,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0213,0.0644
37,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9663,0.0567
38,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,1.0132,0.0639
39,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0002,0.0424
40,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.971,0.0478
41,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.9824,0.048
42,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0405,0.0909
43,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9661,0.0657
44,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,1.0059,0.048
45,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0225,0.0546
46,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.972,0.0689
47,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.988,0.0554
48,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,0.9918,0.0502
49,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9829,0.057
50,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.9611,0.0262
51,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.024,0.0609
52,0.9947,0.0113,0.4171,0.0255,0.5829,0.0255,0.0053,0.0113,0.3644,0.0267,0.6296,0.0174,0.9703,0.0316
53,0.9983,0.0036,0.1885,0.0268,0.8115,0.0268,0.0017,0.0036,0.1188,0.0178,0.8793,0.0164,0.9686,0.0627
54,0.9983,0.0036,0.1744,0.0228,0.8256,0.0228,0.0017,0.0036,0.108,0.0143,0.8903,0.0128,1.0101,0.06
Best version of Logistic Regression is test n°17
-
Mean and St_Dev of the whole Logistic Regression classifier metrics are as follows:
Sensitivity: Mean = 0.9971; St_Dev = 0.0017
Fallout: Mean = 0.26; St_Dev = 0.1123
Specificity: Mean = 0.74; St_Dev = 0.1123
Miss_Rate: Mean = 0.0029; St_Dev = 0.0017
Test_Error: Mean = 0.1971; St_Dev = 0.1195
AUC: Mean = 0.7997; St_Dev = 0.1215
Exec_Time: Mean = 1.0006; St_Dev = 0.0256
#############################
LinearSVC: 18 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.4431,2.9136
2,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.6778,2.8498
3,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.5508,2.864
4,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.5275,2.935
5,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.5949,2.9571
6,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.525,3.0471
7,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.5369,3.0032
8,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.538,2.9477
9,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.4461,2.8695
10,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.6292,2.9802
11,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.5339,3.0587
12,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.507,2.8898
13,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.5843,3.0463
14,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.5877,3.0634
15,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.4121,2.9452
16,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.6436,2.9343
17,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.509,2.8798
18,0.9969,0.0066,0.3483,0.0267,0.6517,0.0267,0.0031,0.0066,0.2726,0.0267,0.7227,0.0242,11.5037,3.0091
Best version of LinearSVC is test n°14
-
Mean and St_Dev of the whole LinearSVC classifier metrics are as follows:
Sensitivity: Mean = 0.9969; St_Dev = 0.0
Fallout: Mean = 0.3483; St_Dev = 0.0
Specificity: Mean = 0.6517; St_Dev = 0.0
Miss_Rate: Mean = 0.0031; St_Dev = 0.0
Test_Error: Mean = 0.2726; St_Dev = 0.0
AUC: Mean = 0.7227; St_Dev = 0.0
Exec_Time: Mean = 11.5417; St_Dev = 0.07
#############################
