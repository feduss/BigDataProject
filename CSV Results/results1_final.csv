Decision_Tree MLLib: 18 versions x 1 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9016,0.0,0.1214,0.0,0.8786,0.0,0.0984,0.0,0.1107,0.0,0.8886,0.0,4.322,0.0
2,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,1.57,0.0
3,0.9397,0.0,0.1233,0.0,0.8767,0.0,0.0603,0.0,0.0954,0.0,0.9032,0.0,1.454,0.0
4,0.9024,0.0,0.1151,0.0,0.8849,0.0,0.0976,0.0,0.1069,0.0,0.8926,0.0,1.449,0.0
5,0.9492,0.0,0.1042,0.0,0.8958,0.0,0.0508,0.0,0.0802,0.0,0.9187,0.0,1.271,0.0
6,0.9397,0.0,0.1233,0.0,0.8767,0.0,0.0603,0.0,0.0954,0.0,0.9032,0.0,1.435,0.0
7,0.896,0.0,0.1095,0.0,0.8905,0.0,0.104,0.0,0.1069,0.0,0.8928,0.0,1.247,0.0
8,0.9492,0.0,0.1042,0.0,0.8958,0.0,0.0508,0.0,0.0802,0.0,0.9187,0.0,1.355,0.0
9,0.9397,0.0,0.1233,0.0,0.8767,0.0,0.0603,0.0,0.0954,0.0,0.9032,0.0,1.519,0.0
10,0.9231,0.0,0.131,0.0,0.869,0.0,0.0769,0.0,0.1069,0.0,0.8919,0.0,1.273,0.0
11,0.9402,0.0,0.1172,0.0,0.8828,0.0,0.0598,0.0,0.0916,0.0,0.9071,0.0,1.076,0.0
12,0.9402,0.0,0.1172,0.0,0.8828,0.0,0.0598,0.0,0.0916,0.0,0.9071,0.0,1.189,0.0
13,0.9322,0.0,0.1181,0.0,0.8819,0.0,0.0678,0.0,0.0954,0.0,0.9034,0.0,1.097,0.0
14,0.9339,0.0,0.0993,0.0,0.9007,0.0,0.0661,0.0,0.084,0.0,0.9153,0.0,1.112,0.0
15,0.9333,0.0,0.1056,0.0,0.8944,0.0,0.0667,0.0,0.0878,0.0,0.9113,0.0,1.115,0.0
16,0.9322,0.0,0.1181,0.0,0.8819,0.0,0.0678,0.0,0.0954,0.0,0.9034,0.0,1.245,0.0
17,0.9339,0.0,0.0993,0.0,0.9007,0.0,0.0661,0.0,0.084,0.0,0.9153,0.0,1.205,0.0
18,0.896,0.0,0.1095,0.0,0.8905,0.0,0.104,0.0,0.1069,0.0,0.8928,0.0,1.235,0.0
Best version of Decision_Tree MLLib is test n°5
With 3 parameters:
Impurity: gini
Max_Depth: 6
Max_Bins: 32
-
Mean and St_Dev of the whole Decision_Tree MLLib classifier metrics are as follows:
Sensitivity: Mean = 0.93; St_Dev = 0.0187
Fallout: Mean = 0.1138; St_Dev = 0.0091
Specificity: Mean = 0.8862; St_Dev = 0.0091
Miss_Rate: Mean = 0.07; St_Dev = 0.0187
Test_Error: Mean = 0.0942; St_Dev = 0.0102
AUC: Mean = 0.9048; St_Dev = 0.0101
Exec_Time: Mean = 1.4538; St_Dev = 0.7312
#############################
Random_Forest MLLib: 36 versions x 1 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9649,0.0,0.1149,0.0,0.8851,0.0,0.0351,0.0,0.0802,0.0,0.9183,0.0,2.207,0.0
2,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,2.702,0.0
3,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,2.209,0.0
4,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,3.023,0.0
5,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,2.805,0.0
6,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,4.366,0.0
7,0.9735,0.0,0.1141,0.0,0.8859,0.0,0.0265,0.0,0.0763,0.0,0.922,0.0,2.364,0.0
8,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,2.997,0.0
9,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,2.673,0.0
10,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,3.641,0.0
11,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,3.081,0.0
12,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,5.107,0.0
13,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,2.438,0.0
14,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,3.535,0.0
15,0.9492,0.0,0.1042,0.0,0.8958,0.0,0.0508,0.0,0.0802,0.0,0.9187,0.0,2.79,0.0
16,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,4.07,0.0
17,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,3.39,0.0
18,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,6.268,0.0
19,0.9735,0.0,0.1141,0.0,0.8859,0.0,0.0265,0.0,0.0763,0.0,0.922,0.0,1.897,0.0
20,0.9737,0.0,0.1081,0.0,0.8919,0.0,0.0263,0.0,0.0725,0.0,0.9259,0.0,2.459,0.0
21,0.982,0.0,0.1192,0.0,0.8808,0.0,0.018,0.0,0.0763,0.0,0.9217,0.0,2.146,0.0
22,0.9561,0.0,0.1216,0.0,0.8784,0.0,0.0439,0.0,0.0878,0.0,0.9106,0.0,2.954,0.0
23,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,2.485,0.0
24,0.982,0.0,0.1192,0.0,0.8808,0.0,0.018,0.0,0.0763,0.0,0.9217,0.0,4.034,0.0
25,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,2.144,0.0
26,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,2.87,0.0
27,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,2.342,0.0
28,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,3.748,0.0
29,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,3.126,0.0
30,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,5.03,0.0
31,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,2.555,0.0
32,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,3.526,0.0
33,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,2.791,0.0
34,0.9576,0.0,0.0972,0.0,0.9028,0.0,0.0424,0.0,0.0725,0.0,0.9264,0.0,4.132,0.0
35,0.9576,0.0,0.0972,0.0,0.9028,0.0,0.0424,0.0,0.0725,0.0,0.9264,0.0,3.442,0.0
36,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,6.124,0.0
Best version of Random_Forest MLLib is test n°20
With 4 parameters:
Impurity: entropy
Max_Depth: 5
Max_Bins: 32
Num_Trees: 100
-
Mean and St_Dev of the whole Random_Forest MLLib classifier metrics are as follows:
Sensitivity: Mean = 0.9597; St_Dev = 0.0075
Fallout: Mean = 0.1105; St_Dev = 0.0058
Specificity: Mean = 0.8895; St_Dev = 0.0058
Miss_Rate: Mean = 0.0403; St_Dev = 0.0075
Test_Error: Mean = 0.0796; St_Dev = 0.0037
AUC: Mean = 0.919; St_Dev = 0.0037
Exec_Time: Mean = 3.2631; St_Dev = 1.0685
#############################
Gradient Boosted Tree: 36 versions x 1 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9174,0.0,0.1135,0.0,0.8865,0.0,0.0826,0.0,0.0992,0.0,0.9,0.0,22.566,0.0
2,0.9106,0.0,0.1079,0.0,0.8921,0.0,0.0894,0.0,0.0992,0.0,0.9002,0.0,52.966,0.0
3,0.9339,0.0,0.0993,0.0,0.9007,0.0,0.0661,0.0,0.084,0.0,0.9153,0.0,23.401,0.0
4,0.9576,0.0,0.0972,0.0,0.9028,0.0,0.0424,0.0,0.0725,0.0,0.9264,0.0,55.254,0.0
5,0.9412,0.0,0.1049,0.0,0.8951,0.0,0.0588,0.0,0.084,0.0,0.915,0.0,25.756,0.0
6,0.9417,0.0,0.0986,0.0,0.9014,0.0,0.0583,0.0,0.0802,0.0,0.919,0.0,58.824,0.0
7,0.9194,0.0,0.0942,0.0,0.9058,0.0,0.0806,0.0,0.0878,0.0,0.9118,0.0,33.482,0.0
8,0.912,0.0,0.0949,0.0,0.9051,0.0,0.088,0.0,0.0916,0.0,0.9081,0.0,85.24,0.0
9,0.95,0.0,0.0915,0.0,0.9085,0.0,0.05,0.0,0.0725,0.0,0.9266,0.0,35.957,0.0
10,0.9421,0.0,0.0922,0.0,0.9078,0.0,0.0579,0.0,0.0763,0.0,0.9229,0.0,89.119,0.0
11,0.9402,0.0,0.1172,0.0,0.8828,0.0,0.0598,0.0,0.0916,0.0,0.9071,0.0,40.457,0.0
12,0.9417,0.0,0.0986,0.0,0.9014,0.0,0.0583,0.0,0.0802,0.0,0.919,0.0,98.264,0.0
13,0.9106,0.0,0.1079,0.0,0.8921,0.0,0.0894,0.0,0.0992,0.0,0.9002,0.0,22.601,0.0
14,0.9106,0.0,0.1079,0.0,0.8921,0.0,0.0894,0.0,0.0992,0.0,0.9002,0.0,59.782,0.0
15,0.9496,0.0,0.0979,0.0,0.9021,0.0,0.0504,0.0,0.0763,0.0,0.9227,0.0,25.929,0.0
16,0.9496,0.0,0.0979,0.0,0.9021,0.0,0.0504,0.0,0.0763,0.0,0.9227,0.0,59.841,0.0
17,0.9333,0.0,0.1056,0.0,0.8944,0.0,0.0667,0.0,0.0878,0.0,0.9113,0.0,28.029,0.0
18,0.9256,0.0,0.1064,0.0,0.8936,0.0,0.0744,0.0,0.0916,0.0,0.9076,0.0,63.971,0.0
19,0.8952,0.0,0.1159,0.0,0.8841,0.0,0.1048,0.0,0.1107,0.0,0.8889,0.0,34.504,0.0
20,0.8952,0.0,0.1159,0.0,0.8841,0.0,0.1048,0.0,0.1107,0.0,0.8889,0.0,84.783,0.0
21,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,38.14,0.0
22,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,94.387,0.0
23,0.9316,0.0,0.1241,0.0,0.8759,0.0,0.0684,0.0,0.0992,0.0,0.8995,0.0,44.606,0.0
24,0.9316,0.0,0.1241,0.0,0.8759,0.0,0.0684,0.0,0.0992,0.0,0.8995,0.0,104.509,0.0
25,0.9106,0.0,0.1079,0.0,0.8921,0.0,0.0894,0.0,0.0992,0.0,0.9002,0.0,24.051,0.0
26,0.9106,0.0,0.1079,0.0,0.8921,0.0,0.0894,0.0,0.0992,0.0,0.9002,0.0,56.253,0.0
27,0.9412,0.0,0.1049,0.0,0.8951,0.0,0.0588,0.0,0.084,0.0,0.915,0.0,25.169,0.0
28,0.9417,0.0,0.0986,0.0,0.9014,0.0,0.0583,0.0,0.0802,0.0,0.919,0.0,59.48,0.0
29,0.9412,0.0,0.1049,0.0,0.8951,0.0,0.0588,0.0,0.084,0.0,0.915,0.0,27.454,0.0
30,0.9412,0.0,0.1049,0.0,0.8951,0.0,0.0588,0.0,0.084,0.0,0.915,0.0,63.395,0.0
31,0.9083,0.0,0.1268,0.0,0.8732,0.0,0.0917,0.0,0.1107,0.0,0.8884,0.0,32.052,0.0
32,0.9083,0.0,0.1268,0.0,0.8732,0.0,0.0917,0.0,0.1107,0.0,0.8884,0.0,76.473,0.0
33,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,36.984,0.0
34,0.9333,0.0,0.1056,0.0,0.8944,0.0,0.0667,0.0,0.0878,0.0,0.9113,0.0,89.065,0.0
35,0.9397,0.0,0.1233,0.0,0.8767,0.0,0.0603,0.0,0.0954,0.0,0.9032,0.0,33.935,0.0
36,0.925,0.0,0.1127,0.0,0.8873,0.0,0.075,0.0,0.0954,0.0,0.9037,0.0,83.82,0.0
Best version of Gradient Boosted Tree is test n°9
With 4 parameters:
Loss: logLoss
Max_Depth: 5
Max_Bins: 32
Num_Iterations: 100
-
Mean and St_Dev of the whole Gradient Boosted Tree classifier metrics are as follows:
Sensitivity: Mean = 0.9309; St_Dev = 0.018
Fallout: Mean = 0.1069; St_Dev = 0.0098
Specificity: Mean = 0.8931; St_Dev = 0.0098
Miss_Rate: Mean = 0.0691; St_Dev = 0.018
Test_Error: Mean = 0.0897; St_Dev = 0.0116
AUC: Mean = 0.9094; St_Dev = 0.0115
Exec_Time: Mean = 52.5139; St_Dev = 25.4031
#############################
Logistic Regression: 54 versions x 1 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,3.033,0.0
2,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,1.805,0.0
3,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,1.786,0.0
4,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.665,0.0
5,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,1.8,0.0
6,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.007,0.0
7,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.715,0.0
8,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,1.821,0.0
9,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,1.693,0.0
10,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.643,0.0
11,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,1.778,0.0
12,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,1.628,0.0
13,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.541,0.0
14,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,1.616,0.0
15,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,1.632,0.0
16,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.696,0.0
17,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,1.723,0.0
18,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,1.64,0.0
19,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.635,0.0
20,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,1.635,0.0
21,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,1.618,0.0
22,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.672,0.0
23,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.054,0.0
24,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.162,0.0
25,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,2.035,0.0
26,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.181,0.0
27,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.099,0.0
28,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,2.054,0.0
29,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.038,0.0
30,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.133,0.0
31,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.982,0.0
32,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.115,0.0
33,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.097,0.0
34,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,2.079,0.0
35,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.074,0.0
36,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.095,0.0
37,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.942,0.0
38,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.101,0.0
39,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.051,0.0
40,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,2.016,0.0
41,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.107,0.0
42,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.097,0.0
43,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,2.113,0.0
44,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.041,0.0
45,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.004,0.0
46,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,2.058,0.0
47,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.015,0.0
48,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.322,0.0
49,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.988,0.0
50,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.09,0.0
51,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.06,0.0
52,1.0,0.0,0.4027,0.0,0.5973,0.0,0.0,0.0,0.3473,0.0,0.6417,0.0,1.985,0.0
53,1.0,0.0,0.1867,0.0,0.8133,0.0,0.0,0.0,0.1183,0.0,0.878,0.0,2.076,0.0
54,1.0,0.0,0.1818,0.0,0.8182,0.0,0.0,0.0,0.1145,0.0,0.8819,0.0,2.098,0.0
Best version of Logistic Regression is test n°21
With 4 parameters:
Max_Iter: 50
Reg_Param: 0.5
Elastic_Net_Param: 0.8
Aggregation_Depth: 3
-
Mean and St_Dev of the whole Logistic Regression classifier metrics are as follows:
Sensitivity: Mean = 1.0; St_Dev = 0.0
Fallout: Mean = 0.2571; St_Dev = 0.104
Specificity: Mean = 0.7429; St_Dev = 0.104
Miss_Rate: Mean = 0.0; St_Dev = 0.0
Test_Error: Mean = 0.1934; St_Dev = 0.1099
AUC: Mean = 0.8005; St_Dev = 0.1134
Exec_Time: Mean = 1.9471; St_Dev = 0.25
#############################
LinearSVC: 18 versions x 1 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.194,0.0
2,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.199,0.0
3,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,29.701,0.0
4,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.879,0.0
5,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.338,0.0
6,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.363,0.0
7,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.047,0.0
8,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.213,0.0
9,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.501,0.0
10,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.198,0.0
11,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.178,0.0
12,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,29.948,0.0
13,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,31.121,0.0
14,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,29.898,0.0
15,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.526,0.0
16,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,29.977,0.0
17,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.636,0.0
18,1.0,0.0,0.351,0.0,0.649,0.0,0.0,0.0,0.2786,0.0,0.7126,0.0,30.544,0.0
Best version of LinearSVC is test n°3
With 3 parameters:
Max_Iter: 50
Reg_Param: 0.1
Aggregation_Depth: 3
-
Mean and St_Dev of the whole LinearSVC classifier metrics are as follows:
Sensitivity: Mean = 1.0; St_Dev = 0.0
Fallout: Mean = 0.351; St_Dev = 0.0
Specificity: Mean = 0.649; St_Dev = 0.0
Miss_Rate: Mean = 0.0; St_Dev = 0.0
Test_Error: Mean = 0.2786; St_Dev = 0.0
AUC: Mean = 0.7126; St_Dev = 0.0
Exec_Time: Mean = 30.3034; St_Dev = 0.3552
#############################
