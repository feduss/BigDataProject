Decision_Tree MLLib: 18 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9886,0.0,0.1056,0.0,0.8944,0.0,0.0114,0.0,0.0443,0.0,0.9613,0.0,0.9873,0.6808
2,0.9846,0.0,0.137,0.0,0.863,0.0,0.0154,0.0,0.0591,0.0,0.9484,0.0,0.6676,0.0985
3,0.9848,0.0,0.1189,0.0,0.8811,0.0,0.0152,0.0,0.0517,0.0,0.9538,0.0,0.6742,0.0848
4,0.9886,0.0,0.1056,0.0,0.8944,0.0,0.0114,0.0,0.0443,0.0,0.9613,0.0,0.6871,0.1123
5,0.9771,0.0,0.1389,0.0,0.8611,0.0,0.0229,0.0,0.064,0.0,0.9407,0.0,0.6976,0.0845
6,0.9774,0.0,0.1143,0.0,0.8857,0.0,0.0226,0.0,0.0542,0.0,0.9479,0.0,0.7348,0.083
7,0.9704,0.0,0.1029,0.0,0.8971,0.0,0.0296,0.0,0.0542,0.0,0.9439,0.0,0.6659,0.079
8,0.9847,0.0,0.131,0.0,0.869,0.0,0.0153,0.0,0.0567,0.0,0.9502,0.0,0.7628,0.148
9,0.9812,0.0,0.1071,0.0,0.8929,0.0,0.0188,0.0,0.0493,0.0,0.9536,0.0,0.8411,0.1278
10,0.9811,0.0,0.1135,0.0,0.8865,0.0,0.0189,0.0,0.0517,0.0,0.9518,0.0,0.6407,0.1199
11,0.9665,0.0,0.1168,0.0,0.8832,0.0,0.0335,0.0,0.0616,0.0,0.9364,0.0,0.6696,0.131
12,0.9739,0.0,0.1087,0.0,0.8913,0.0,0.0261,0.0,0.0542,0.0,0.9459,0.0,0.6921,0.1614
13,0.9774,0.0,0.1206,0.0,0.8794,0.0,0.0226,0.0,0.0567,0.0,0.9461,0.0,0.6751,0.14
14,0.9665,0.0,0.1168,0.0,0.8832,0.0,0.0335,0.0,0.0616,0.0,0.9364,0.0,0.6506,0.0581
15,0.9811,0.0,0.1135,0.0,0.8865,0.0,0.0189,0.0,0.0517,0.0,0.9518,0.0,0.7257,0.1089
16,0.9631,0.0,0.1111,0.0,0.8889,0.0,0.0369,0.0,0.0616,0.0,0.9344,0.0,0.6842,0.1084
17,0.9453,0.0,0.1288,0.0,0.8712,0.0,0.0547,0.0,0.0788,0.0,0.9115,0.0,0.6882,0.1137
18,0.9564,0.0,0.0992,0.0,0.9008,0.0,0.0436,0.0,0.0616,0.0,0.9303,0.0,0.8095,0.1138
Best version of Decision_Tree MLLib is test n°3
With 3 parameters:
Impurity: gini
Max_Depth: 5
Max_Bins: 128
-
Mean and St_Dev of the whole Decision_Tree MLLib classifier metrics are as follows:
Sensitivity: Mean = 0.9749; St_Dev = 0.0117
Fallout: Mean = 0.1161; St_Dev = 0.0114
Specificity: Mean = 0.8839; St_Dev = 0.0114
Miss_Rate: Mean = 0.0251; St_Dev = 0.0117
Test_Error: Mean = 0.0565; St_Dev = 0.008
AUC: Mean = 0.9448; St_Dev = 0.012
Exec_Time: Mean = 0.7197; St_Dev = 0.0855
#############################
Random_Forest MLLib: 36 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9969,0.003,0.1126,0.0036,0.8874,0.0036,0.0031,0.003,0.0424,0.003,0.9672,0.0038,0.9852,0.153
2,0.9973,0.0018,0.1125,0.0032,0.8875,0.0032,0.0027,0.0018,0.0421,0.0018,0.9676,0.0021,1.368,0.0922
3,0.9939,0.002,0.112,0.0057,0.888,0.0057,0.0061,0.002,0.0439,0.0028,0.9645,0.0028,1.0929,0.0895
4,0.9962,0.0025,0.1115,0.0037,0.8885,0.0037,0.0038,0.0025,0.0424,0.0019,0.9668,0.0025,1.6238,0.044
5,0.9989,0.0018,0.1098,0.0029,0.8902,0.0029,0.0011,0.0018,0.0401,0.0017,0.9699,0.002,1.3697,0.0701
6,0.9977,0.002,0.1106,0.0046,0.8894,0.0046,0.0023,0.002,0.0411,0.0023,0.9685,0.0025,2.226,0.164
7,0.9966,0.0022,0.1114,0.0038,0.8886,0.0038,0.0034,0.0022,0.0421,0.0018,0.9672,0.0022,1.1562,0.1374
8,0.9973,0.0018,0.1107,0.0034,0.8893,0.0034,0.0027,0.0018,0.0414,0.0016,0.9682,0.0018,1.7409,0.1178
9,0.9947,0.0032,0.1075,0.0039,0.8925,0.0039,0.0053,0.0032,0.0416,0.0014,0.9665,0.0024,1.267,0.0794
10,0.9943,0.002,0.1107,0.0041,0.8893,0.0041,0.0057,0.002,0.0431,0.0021,0.9653,0.0024,2.0425,0.1177
11,0.9958,0.0028,0.1067,0.0053,0.8933,0.0053,0.0042,0.0028,0.0406,0.0029,0.9679,0.0034,1.6825,0.0986
12,0.997,0.0016,0.1071,0.0044,0.8929,0.0044,0.003,0.0016,0.0401,0.0021,0.9689,0.002,2.775,0.1641
13,0.9962,0.0025,0.1035,0.0046,0.8965,0.0046,0.0038,0.0025,0.0392,0.0025,0.9692,0.0029,1.3562,0.1768
14,0.9962,0.0018,0.1054,0.0038,0.8946,0.0038,0.0038,0.0018,0.0399,0.0016,0.9686,0.0017,2.1223,0.102
15,0.9939,0.0037,0.1033,0.0061,0.8967,0.0061,0.0061,0.0037,0.0404,0.0039,0.967,0.0046,1.5581,0.117
16,0.992,0.0012,0.105,0.0041,0.8951,0.0041,0.008,0.0012,0.0421,0.0014,0.9647,0.0012,2.5317,0.1639
17,0.9947,0.0027,0.0988,0.0058,0.9012,0.0058,0.0053,0.0027,0.0382,0.0033,0.969,0.0037,1.9524,0.0879
18,0.9962,0.0018,0.1004,0.0033,0.8996,0.0033,0.0038,0.0018,0.0379,0.0017,0.9701,0.002,3.5343,0.244
19,0.9992,0.0024,0.1145,0.0036,0.8854,0.0036,0.0008,0.0024,0.0419,0.0016,0.9688,0.0022,0.9402,0.1245
20,1.0,0.0,0.1162,0.0034,0.8838,0.0034,0.0,0.0,0.0421,0.0014,0.969,0.001,1.3403,0.0816
21,0.9962,0.0018,0.1164,0.0029,0.8836,0.0029,0.0038,0.0018,0.0443,0.0016,0.9654,0.002,1.1047,0.1309
22,0.997,0.0016,0.115,0.0027,0.885,0.0027,0.003,0.0016,0.0433,0.0017,0.9665,0.002,1.6612,0.1056
23,0.9966,0.0012,0.1133,0.0044,0.8867,0.0044,0.0034,0.0012,0.0429,0.0021,0.9667,0.002,1.3541,0.124
24,0.9977,0.002,0.1142,0.0027,0.8858,0.0027,0.0023,0.002,0.0426,0.0012,0.9675,0.0017,2.1219,0.1062
25,0.9985,0.0027,0.1111,0.0039,0.8889,0.0039,0.0015,0.0027,0.0409,0.0024,0.9691,0.003,1.088,0.0812
26,0.9996,0.0012,0.1121,0.003,0.8879,0.003,0.0004,0.0012,0.0406,0.0013,0.9699,0.0013,1.6333,0.1336
27,0.9962,0.0025,0.1103,0.004,0.8897,0.004,0.0038,0.0025,0.0419,0.002,0.9672,0.0025,1.246,0.0719
28,0.9962,0.0,0.1121,0.0041,0.8879,0.0041,0.0038,0.0,0.0426,0.0016,0.9667,0.0012,1.9859,0.0725
29,0.997,0.0016,0.1126,0.0058,0.8874,0.0058,0.003,0.0016,0.0424,0.0023,0.9672,0.0019,1.6366,0.0967
30,0.9977,0.002,0.113,0.0041,0.887,0.0041,0.0023,0.002,0.0421,0.0014,0.9678,0.0016,2.6657,0.1055
31,0.9981,0.002,0.1099,0.0041,0.8901,0.0041,0.0019,0.002,0.0406,0.0021,0.9691,0.0023,1.2739,0.0785
32,0.9985,0.002,0.1086,0.0048,0.8914,0.0048,0.0015,0.002,0.0399,0.0023,0.9698,0.0024,1.9629,0.114
33,0.9958,0.0012,0.1079,0.0042,0.8921,0.0042,0.0042,0.0012,0.0411,0.0017,0.9675,0.0015,1.4747,0.0908
34,0.9966,0.0012,0.1072,0.0031,0.8928,0.0031,0.0034,0.0012,0.0404,0.0013,0.9685,0.0013,2.3838,0.1207
35,0.9958,0.0012,0.1055,0.005,0.8945,0.005,0.0042,0.0012,0.0401,0.0024,0.9682,0.0022,1.9277,0.1217
36,0.9962,0.0018,0.1073,0.0032,0.8927,0.0032,0.0038,0.0018,0.0406,0.0017,0.9681,0.0021,3.2632,0.1536
Best version of Random_Forest MLLib is test n°17
With 4 parameters:
Impurity: gini
Max_Depth: 7
Max_Bins: 128
Num_Trees: 100
-
Mean and St_Dev of the whole Random_Forest MLLib classifier metrics are as follows:
Sensitivity: Mean = 0.9966; St_Dev = 0.0017
Fallout: Mean = 0.1096; St_Dev = 0.0042
Specificity: Mean = 0.8904; St_Dev = 0.0042
Miss_Rate: Mean = 0.0034; St_Dev = 0.0017
Test_Error: Mean = 0.0414; St_Dev = 0.0015
AUC: Mean = 0.9678; St_Dev = 0.0014
Exec_Time: Mean = 1.7625; St_Dev = 0.6248
#############################
Gradient Boosted Tree: 36 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9924,0.0,0.1049,0.0,0.8951,0.0,0.0076,0.0,0.0419,0.0,0.9651,0.0,8.8419,0.2378
2,0.9886,0.0,0.1056,0.0,0.8944,0.0,0.0114,0.0,0.0443,0.0,0.9613,0.0,20.3667,0.2753
3,0.985,0.0,0.1,0.0,0.9,0.0,0.015,0.0,0.0443,0.0,0.9593,0.0,9.1777,0.162
4,0.985,0.0,0.0935,0.0,0.9065,0.0,0.015,0.0,0.0419,0.0,0.9611,0.0,21.3041,0.3055
5,0.9887,0.0,0.0993,0.0,0.9007,0.0,0.0113,0.0,0.0419,0.0,0.9631,0.0,10.1123,0.0786
6,0.985,0.0,0.1,0.0,0.9,0.0,0.015,0.0,0.0443,0.0,0.9593,0.0,22.9412,0.7591
7,0.9887,0.0,0.0993,0.0,0.9007,0.0,0.0113,0.0,0.0419,0.0,0.9631,0.0,13.5823,0.239
8,0.9887,0.0,0.0993,0.0,0.9007,0.0,0.0113,0.0,0.0419,0.0,0.9631,0.0,35.2699,0.4756
9,0.9848,0.0,0.1189,0.0,0.8811,0.0,0.0152,0.0,0.0517,0.0,0.9538,0.0,13.9964,0.3144
10,0.9848,0.0,0.1127,0.0,0.8873,0.0,0.0152,0.0,0.0493,0.0,0.9556,0.0,34.2309,0.7062
11,0.9811,0.0,0.1135,0.0,0.8865,0.0,0.0189,0.0,0.0517,0.0,0.9518,0.0,15.7013,0.2735
12,0.9812,0.0,0.1071,0.0,0.8929,0.0,0.0188,0.0,0.0493,0.0,0.9536,0.0,38.1408,0.2758
13,0.9885,0.0,0.1181,0.0,0.8819,0.0,0.0115,0.0,0.0493,0.0,0.9577,0.0,8.696,0.2045
14,0.9885,0.0,0.1241,0.0,0.8759,0.0,0.0115,0.0,0.0517,0.0,0.9559,0.0,20.5095,0.3654
15,0.9812,0.0,0.1071,0.0,0.8929,0.0,0.0188,0.0,0.0493,0.0,0.9536,0.0,9.3841,0.2627
16,0.9812,0.0,0.1071,0.0,0.8929,0.0,0.0188,0.0,0.0493,0.0,0.9536,0.0,21.3861,0.3427
17,0.9812,0.0,0.1071,0.0,0.8929,0.0,0.0188,0.0,0.0493,0.0,0.9536,0.0,10.2324,0.2297
18,0.9812,0.0,0.1071,0.0,0.8929,0.0,0.0188,0.0,0.0493,0.0,0.9536,0.0,22.9434,0.5471
19,0.9886,0.0,0.1056,0.0,0.8944,0.0,0.0114,0.0,0.0443,0.0,0.9613,0.0,13.3276,0.3141
20,0.9886,0.0,0.1056,0.0,0.8944,0.0,0.0114,0.0,0.0443,0.0,0.9613,0.0,34.2034,0.8283
21,0.9846,0.0,0.137,0.0,0.863,0.0,0.0154,0.0,0.0591,0.0,0.9484,0.0,14.0564,0.2925
22,0.9846,0.0,0.137,0.0,0.863,0.0,0.0154,0.0,0.0591,0.0,0.9484,0.0,34.9098,0.2079
23,0.9848,0.0,0.1127,0.0,0.8873,0.0,0.0152,0.0,0.0493,0.0,0.9556,0.0,15.8698,0.2827
24,0.9848,0.0,0.1127,0.0,0.8873,0.0,0.0152,0.0,0.0493,0.0,0.9556,0.0,38.4513,0.6753
25,0.9885,0.0,0.1241,0.0,0.8759,0.0,0.0115,0.0,0.0517,0.0,0.9559,0.0,8.9937,0.2767
26,0.9885,0.0,0.1181,0.0,0.8819,0.0,0.0115,0.0,0.0493,0.0,0.9577,0.0,19.4278,0.7986
27,0.9848,0.0,0.1127,0.0,0.8873,0.0,0.0152,0.0,0.0493,0.0,0.9556,0.0,7.8017,0.5366
28,0.9848,0.0,0.1127,0.0,0.8873,0.0,0.0152,0.0,0.0493,0.0,0.9556,0.0,18.1202,0.3963
29,0.9848,0.0,0.1127,0.0,0.8873,0.0,0.0152,0.0,0.0493,0.0,0.9556,0.0,8.5467,0.1687
30,0.9848,0.0,0.1127,0.0,0.8873,0.0,0.0152,0.0,0.0493,0.0,0.9556,0.0,19.6424,0.1779
31,0.985,0.0,0.0935,0.0,0.9065,0.0,0.015,0.0,0.0419,0.0,0.9611,0.0,10.7288,0.5122
32,0.9886,0.0,0.1056,0.0,0.8944,0.0,0.0114,0.0,0.0443,0.0,0.9613,0.0,28.7854,0.5959
33,0.9811,0.0,0.1135,0.0,0.8865,0.0,0.0189,0.0,0.0517,0.0,0.9518,0.0,12.8984,0.9618
34,0.9848,0.0,0.1127,0.0,0.8873,0.0,0.0152,0.0,0.0493,0.0,0.9556,0.0,32.5566,0.4686
35,0.9774,0.0,0.1206,0.0,0.8794,0.0,0.0226,0.0,0.0567,0.0,0.9461,0.0,14.9529,0.2088
36,0.9774,0.0,0.1143,0.0,0.8857,0.0,0.0226,0.0,0.0542,0.0,0.9479,0.0,36.8622,0.1318
Best version of Gradient Boosted Tree is test n°0
With 4 parameters:
Loss: leastAbsoluteError
Max_Depth: 5
Max_Bins: 128
Num_Iterations: 100
-
Mean and St_Dev of the whole Gradient Boosted Tree classifier metrics are as follows:
Sensitivity: Mean = 0.9851; St_Dev = 0.0034
Fallout: Mean = 0.1108; St_Dev = 0.01
Specificity: Mean = 0.8892; St_Dev = 0.01
Miss_Rate: Mean = 0.0149; St_Dev = 0.0034
Test_Error: Mean = 0.0485; St_Dev = 0.0046
AUC: Mean = 0.9564; St_Dev = 0.0047
Exec_Time: Mean = 19.6376; St_Dev = 9.9999
#############################
Logistic Regression: 54 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,1.1903,0.3944
2,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.0339,0.0673
3,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0138,0.0744
4,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9507,0.0934
5,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.015,0.0708
6,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0685,0.1092
7,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9745,0.0781
8,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.0126,0.055
9,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0058,0.1231
10,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9744,0.0635
11,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,0.9934,0.0896
12,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0076,0.0714
13,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9344,0.1095
14,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,0.9998,0.1427
15,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0031,0.0716
16,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9083,0.0692
17,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.0057,0.0879
18,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0734,0.1132
19,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9719,0.0901
20,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.024,0.1252
21,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,0.9598,0.0383
22,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9542,0.0285
23,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.0256,0.0947
24,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0277,0.0785
25,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9645,0.0619
26,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.0177,0.0391
27,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,0.9921,0.0734
28,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9516,0.0342
29,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,0.9759,0.0637
30,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0078,0.0347
31,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9536,0.0517
32,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,0.9789,0.0673
33,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0573,0.1156
34,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9814,0.0611
35,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.016,0.0644
36,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,0.9835,0.0704
37,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9823,0.1052
38,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,0.9714,0.0258
39,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0254,0.0979
40,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9821,0.0816
41,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.0014,0.0373
42,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0135,0.064
43,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9662,0.0619
44,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,0.9665,0.0405
45,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0039,0.0777
46,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9656,0.0704
47,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,1.0087,0.0592
48,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,0.988,0.0583
49,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9597,0.0521
50,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,0.9874,0.1011
51,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,0.9966,0.0556
52,1.0,0.0,0.1667,0.0,0.8333,0.0,0.0,0.0,0.064,0.0,0.9529,0.0,0.9276,0.0603
53,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,0.9811,0.0449
54,1.0,0.0,0.1216,0.0,0.8784,0.0,0.0,0.0,0.0443,0.0,0.9674,0.0,1.0097,0.072
Best version of Logistic Regression is test n°20
With 4 parameters:
Max_Iter: 50
Reg_Param: 0.5
Elastic_Net_Param: 0.8
Aggregation_Depth: 3
-
Mean and St_Dev of the whole Logistic Regression classifier metrics are as follows:
Sensitivity: Mean = 1.0; St_Dev = 0.0
Fallout: Mean = 0.1443; St_Dev = 0.0186
Specificity: Mean = 0.8557; St_Dev = 0.0186
Miss_Rate: Mean = 0.0; St_Dev = 0.0
Test_Error: Mean = 0.0542; St_Dev = 0.0081
AUC: Mean = 0.9601; St_Dev = 0.006
Exec_Time: Mean = 0.9953; St_Dev = 0.0423
#############################
LinearSVC: 18 versions x 10 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.7235,0.2562
2,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.5441,0.1826
3,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.7157,0.1362
4,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.5976,0.2213
5,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.7423,0.2045
6,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.5122,0.2772
7,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.6905,0.258
8,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.7035,0.196
9,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.6718,0.2247
10,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.5741,0.2586
11,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.7015,0.2292
12,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.6809,0.1362
13,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.6085,0.1577
14,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.6582,0.2058
15,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.8085,0.2969
16,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.6999,0.1892
17,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.6771,0.2489
18,1.0,0.0,0.1447,0.0,0.8553,0.0,0.0,0.0,0.0542,0.0,0.9601,0.0,6.7451,0.2767
Best version of LinearSVC is test n°5
With 3 parameters:
Max_Iter: 50
Reg_Param: 0.3
Aggregation_Depth: 3
-
Mean and St_Dev of the whole LinearSVC classifier metrics are as follows:
Sensitivity: Mean = 1.0; St_Dev = 0.0
Fallout: Mean = 0.1447; St_Dev = 0.0
Specificity: Mean = 0.8553; St_Dev = 0.0
Miss_Rate: Mean = 0.0; St_Dev = 0.0
Test_Error: Mean = 0.0542; St_Dev = 0.0
AUC: Mean = 0.9601; St_Dev = 0.0
Exec_Time: Mean = 6.6697; St_Dev = 0.0758
#############################
