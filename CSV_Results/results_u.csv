Decision_Tree: 18 versions x 5 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9016,0.0,0.1214,0.0,0.8786,0.0,0.0984,0.0,0.1107,0.0,0.8886,0.0,4.4594,1.7681
2,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,3.623,0.1526
3,0.9397,0.0,0.1233,0.0,0.8767,0.0,0.0603,0.0,0.0954,0.0,0.9032,0.0,3.5136,0.0932
4,0.9024,0.0,0.1151,0.0,0.8849,0.0,0.0976,0.0,0.1069,0.0,0.8926,0.0,3.5052,0.1224
5,0.9492,0.0,0.1042,0.0,0.8958,0.0,0.0508,0.0,0.0802,0.0,0.9187,0.0,3.5012,0.1079
6,0.9397,0.0,0.1233,0.0,0.8767,0.0,0.0603,0.0,0.0954,0.0,0.9032,0.0,3.5146,0.0737
7,0.896,0.0,0.1095,0.0,0.8905,0.0,0.104,0.0,0.1069,0.0,0.8928,0.0,3.4432,0.0328
8,0.9492,0.0,0.1042,0.0,0.8958,0.0,0.0508,0.0,0.0802,0.0,0.9187,0.0,3.4066,0.066
9,0.9397,0.0,0.1233,0.0,0.8767,0.0,0.0603,0.0,0.0954,0.0,0.9032,0.0,3.5492,0.0669
10,0.9231,0.0,0.131,0.0,0.869,0.0,0.0769,0.0,0.1069,0.0,0.8919,0.0,3.3588,0.1253
11,0.9402,0.0,0.1172,0.0,0.8828,0.0,0.0598,0.0,0.0916,0.0,0.9071,0.0,3.3694,0.053
12,0.9402,0.0,0.1172,0.0,0.8828,0.0,0.0598,0.0,0.0916,0.0,0.9071,0.0,3.442,0.037
13,0.9322,0.0,0.1181,0.0,0.8819,0.0,0.0678,0.0,0.0954,0.0,0.9034,0.0,3.4222,0.0814
14,0.9339,0.0,0.0993,0.0,0.9007,0.0,0.0661,0.0,0.084,0.0,0.9153,0.0,3.4604,0.0665
15,0.9333,0.0,0.1056,0.0,0.8944,0.0,0.0667,0.0,0.0878,0.0,0.9113,0.0,3.4128,0.0561
16,0.9322,0.0,0.1181,0.0,0.8819,0.0,0.0678,0.0,0.0954,0.0,0.9034,0.0,3.4446,0.0878
17,0.9339,0.0,0.0993,0.0,0.9007,0.0,0.0661,0.0,0.084,0.0,0.9153,0.0,3.4462,0.0603
18,0.896,0.0,0.1095,0.0,0.8905,0.0,0.104,0.0,0.1069,0.0,0.8928,0.0,3.5798,0.0995
Best version of Decision_Tree is test n°8
With 3 parameters:
Impurity: gini
Max_Depth: 7
Max_Bins: 32
-
Mean and St_Dev of the whole Decision_Tree classifier metrics are as follows:
Sensitivity: Mean = 0.93; St_Dev = 0.0187
Fallout: Mean = 0.1138; St_Dev = 0.0091
Specificity: Mean = 0.8862; St_Dev = 0.0091
Miss_Rate: Mean = 0.07; St_Dev = 0.0187
Test_Error: Mean = 0.0942; St_Dev = 0.0102
AUC: Mean = 0.9048; St_Dev = 0.0101
Exec_Time: Mean = 3.5251; St_Dev = 0.2434
#############################
Random_Forest: 36 versions x 5 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,4.2438,0.0953
2,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,4.704,0.1128
3,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,4.2764,0.0433
4,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,5.1524,0.0726
5,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,4.7016,0.0987
6,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,6.0598,0.0684
7,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,4.7008,0.2248
8,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,5.1982,0.07
9,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,4.6654,0.1246
10,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,5.691,0.0734
11,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,5.1306,0.1105
12,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,6.7692,0.1403
13,0.958,0.0,0.0909,0.0,0.9091,0.0,0.042,0.0,0.0687,0.0,0.9303,0.0,4.7372,0.0484
14,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,5.6516,0.0682
15,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,4.901,0.0294
16,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,6.1398,0.0943
17,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,5.4416,0.0854
18,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,7.5686,0.1497
19,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,4.1426,0.0764
20,0.9735,0.0,0.1141,0.0,0.8859,0.0,0.0265,0.0,0.0763,0.0,0.922,0.0,4.7494,0.1378
21,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,4.3868,0.0752
22,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,5.0358,0.0776
23,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,4.7168,0.1018
24,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,6.0108,0.1562
25,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,4.431,0.0777
26,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,5.2272,0.1531
27,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,4.6674,0.0461
28,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,5.7072,0.049
29,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,5.1742,0.0825
30,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,6.9924,0.1258
31,0.9576,0.0,0.0972,0.0,0.9028,0.0,0.0424,0.0,0.0725,0.0,0.9264,0.0,4.7704,0.0605
32,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,5.7926,0.1076
33,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,4.9756,0.0571
34,0.9569,0.0,0.1096,0.0,0.8904,0.0,0.0431,0.0,0.0802,0.0,0.9185,0.0,6.3108,0.0522
35,0.9573,0.0,0.1034,0.0,0.8966,0.0,0.0427,0.0,0.0763,0.0,0.9224,0.0,5.5686,0.0999
36,0.9565,0.0,0.1156,0.0,0.8844,0.0,0.0435,0.0,0.084,0.0,0.9146,0.0,7.8096,0.1563
Best version of Random_Forest is test n°13
With 4 parameters:
Impurity: gini
Max_Depth: 6
Max_Bins: 128
Num_Trees: 200
-
Mean and St_Dev of the whole Random_Forest classifier metrics are as follows:
Sensitivity: Mean = 0.9575; St_Dev = 0.0028
Fallout: Mean = 0.1081; St_Dev = 0.006
Specificity: Mean = 0.8919; St_Dev = 0.006
Miss_Rate: Mean = 0.0425; St_Dev = 0.0028
Test_Error: Mean = 0.0791; St_Dev = 0.0037
AUC: Mean = 0.9196; St_Dev = 0.0038
Exec_Time: Mean = 5.3389; St_Dev = 0.9043
#############################
Gradient_Boosted_Tree: 12 versions x 5 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.9174,0.0,0.1135,0.0,0.8865,0.0,0.0826,0.0,0.0992,0.0,0.9,0.0,21.4176,0.23
2,0.9106,0.0,0.1079,0.0,0.8921,0.0,0.0894,0.0,0.0992,0.0,0.9002,0.0,49.5846,0.1212
3,0.9339,0.0,0.0993,0.0,0.9007,0.0,0.0661,0.0,0.084,0.0,0.9153,0.0,22.3334,0.0836
4,0.9576,0.0,0.0972,0.0,0.9028,0.0,0.0424,0.0,0.0725,0.0,0.9264,0.0,51.588,0.1127
5,0.9412,0.0,0.1049,0.0,0.8951,0.0,0.0588,0.0,0.084,0.0,0.915,0.0,24.3008,0.1771
6,0.9417,0.0,0.0986,0.0,0.9014,0.0,0.0583,0.0,0.0802,0.0,0.919,0.0,55.6164,0.129
7,0.9194,0.0,0.0942,0.0,0.9058,0.0,0.0806,0.0,0.0878,0.0,0.9118,0.0,31.5836,0.1787
8,0.912,0.0,0.0949,0.0,0.9051,0.0,0.088,0.0,0.0916,0.0,0.9081,0.0,79.793,0.1634
9,0.95,0.0,0.0915,0.0,0.9085,0.0,0.05,0.0,0.0725,0.0,0.9266,0.0,33.52,0.2729
10,0.9421,0.0,0.0922,0.0,0.9078,0.0,0.0579,0.0,0.0763,0.0,0.9229,0.0,81.3066,0.2952
11,0.9402,0.0,0.1172,0.0,0.8828,0.0,0.0598,0.0,0.0916,0.0,0.9071,0.0,36.6694,0.2548
12,0.9417,0.0,0.0986,0.0,0.9014,0.0,0.0583,0.0,0.0802,0.0,0.919,0.0,89.9162,0.1544
Best version of Gradient_Boosted_Tree is test n°9
With 3 parameters:
Max_Depth: 5
Max_Bins: 32
Num_Iterations: 100
-
Mean and St_Dev of the whole Gradient_Boosted_Tree classifier metrics are as follows:
Sensitivity: Mean = 0.934; St_Dev = 0.0154
Fallout: Mean = 0.1008; St_Dev = 0.0083
Specificity: Mean = 0.8992; St_Dev = 0.0083
Miss_Rate: Mean = 0.066; St_Dev = 0.0154
Test_Error: Mean = 0.0849; St_Dev = 0.0092
AUC: Mean = 0.9143; St_Dev = 0.0091
Exec_Time: Mean = 48.1358; St_Dev = 24.2803
#############################
Logistic_Regression: 54 versions x 5 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,0.5743,0.0,0.3684,0.0,0.6316,0.0,0.4257,0.0,0.4008,0.0,0.6013,0.0,4.8136,2.1058
2,0.5743,0.0,0.3684,0.0,0.6316,0.0,0.4257,0.0,0.4008,0.0,0.6013,0.0,4.1514,0.2096
3,0.5743,0.0,0.3684,0.0,0.6316,0.0,0.4257,0.0,0.4008,0.0,0.6013,0.0,4.1748,0.0879
4,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.8176,0.0997
5,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.2768,0.0952
6,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.4842,0.1896
7,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.0524,0.1433
8,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.4406,0.1177
9,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.6308,0.1813
10,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.574,0.0868
11,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.1126,0.14
12,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.235,0.3808
13,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.3946,0.2268
14,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.7458,0.1829
15,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.8688,0.1937
16,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.5554,0.13
17,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.0086,0.1093
18,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.0834,0.21
19,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.6578,0.2926
20,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.9156,0.1305
21,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.0656,0.1841
22,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.1974,0.148
23,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.899,0.1521
24,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.9584,0.1539
25,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.0756,0.2379
26,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.7266,0.3236
27,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.7866,0.2674
28,0.5743,0.0,0.3684,0.0,0.6316,0.0,0.4257,0.0,0.4008,0.0,0.6013,0.0,5.4772,0.3045
29,0.5743,0.0,0.3684,0.0,0.6316,0.0,0.4257,0.0,0.4008,0.0,0.6013,0.0,6.6822,0.1421
30,0.5743,0.0,0.3684,0.0,0.6316,0.0,0.4257,0.0,0.4008,0.0,0.6013,0.0,6.8358,0.3253
31,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.7766,0.1456
32,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.38,0.2449
33,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.3244,0.1588
34,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.9186,0.1952
35,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.3484,0.228
36,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.482,0.1626
37,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.5574,0.0684
38,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.836,0.1236
39,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.0036,0.1284
40,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.26,0.1268
41,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.7058,0.1485
42,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.7414,0.1719
43,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.7448,0.1478
44,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.0852,0.1575
45,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.1996,0.2547
46,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,2.582,0.1061
47,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.1652,0.2038
48,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.0996,0.1246
49,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.3292,0.2288
50,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.9398,0.1794
51,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,4.2278,0.3594
52,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.0942,0.2145
53,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.5326,0.089
54,0.5417,0.0,0.4153,0.0,0.5847,0.0,0.4583,0.0,0.4389,0.0,0.5626,0.0,3.8186,0.1562
Best version of Logistic_Regression is test n°2
With 4 parameters:
Max_Iter: 50
Reg_Param: 0.1
Elastic_Net_Param: 0.6
Aggregation_Depth: 2
-
Mean and St_Dev of the whole Logistic_Regression classifier metrics are as follows:
Sensitivity: Mean = 0.5453; St_Dev = 0.0103
Fallout: Mean = 0.4101; St_Dev = 0.0149
Specificity: Mean = 0.5899; St_Dev = 0.0149
Miss_Rate: Mean = 0.4547; St_Dev = 0.0103
Test_Error: Mean = 0.4347; St_Dev = 0.0121
AUC: Mean = 0.5669; St_Dev = 0.0123
Exec_Time: Mean = 3.3861; St_Dev = 0.8966
#############################
Linear_SVC: 18 versions x 5 iterations
Version,Sensitivity(Mean),Sensitivity(StDev),Fallout(Mean),Fallout(StDev),Specificity(Mean),Specificity(StDev),Miss_Rate(Mean),Miss_Rate(StDev),Test_Error(Mean),Test_Error(StDev),AUC(Mean),AUC(StDev),Exec_Time(Mean),Exec_Time(StDev)
1,1.0,0.0,0.2819,0.0,0.7181,0.0,0.0,0.0,0.2023,0.0,0.7913,0.0,76.7104,2.047
2,1.0,0.0,0.2819,0.0,0.7181,0.0,0.0,0.0,0.2023,0.0,0.7913,0.0,77.3452,0.1522
3,1.0,0.0,0.2819,0.0,0.7181,0.0,0.0,0.0,0.2023,0.0,0.7913,0.0,79.2336,2.0233
4,0.9683,0.0,0.3317,0.0,0.6683,0.0,0.0317,0.0,0.2595,0.0,0.7328,0.0,67.0918,0.1522
5,0.9683,0.0,0.3317,0.0,0.6683,0.0,0.0317,0.0,0.2595,0.0,0.7328,0.0,68.9158,0.2941
6,0.9683,0.0,0.3317,0.0,0.6683,0.0,0.0317,0.0,0.2595,0.0,0.7328,0.0,68.7832,0.5892
7,0.7273,0.0,0.3374,0.0,0.6626,0.0,0.2727,0.0,0.313,0.0,0.6835,0.0,79.897,1.6051
8,0.7273,0.0,0.3374,0.0,0.6626,0.0,0.2727,0.0,0.313,0.0,0.6835,0.0,91.1898,7.1024
9,0.7273,0.0,0.3374,0.0,0.6626,0.0,0.2727,0.0,0.313,0.0,0.6835,0.0,101.6788,0.9116
10,0.9873,0.0,0.2678,0.0,0.7322,0.0,0.0127,0.0,0.1908,0.0,0.8034,0.0,204.659,29.1629
11,0.9873,0.0,0.2678,0.0,0.7322,0.0,0.0127,0.0,0.1908,0.0,0.8034,0.0,164.1992,16.0202
12,0.9873,0.0,0.2678,0.0,0.7322,0.0,0.0127,0.0,0.1908,0.0,0.8034,0.0,152.7394,0.1827
13,0.9692,0.0,0.3249,0.0,0.6751,0.0,0.0308,0.0,0.2519,0.0,0.7406,0.0,130.2498,0.3226
14,0.9692,0.0,0.3249,0.0,0.6751,0.0,0.0308,0.0,0.2519,0.0,0.7406,0.0,133.0188,0.3112
15,0.9692,0.0,0.3249,0.0,0.6751,0.0,0.0308,0.0,0.2519,0.0,0.7406,0.0,133.705,0.8397
16,0.7048,0.0,0.3376,0.0,0.6624,0.0,0.2952,0.0,0.3206,0.0,0.6765,0.0,169.635,0.3186
17,0.7048,0.0,0.3376,0.0,0.6624,0.0,0.2952,0.0,0.3206,0.0,0.6765,0.0,172.978,0.2077
18,0.7048,0.0,0.3376,0.0,0.6624,0.0,0.2952,0.0,0.3206,0.0,0.6765,0.0,174.4188,2.8048
Best version of Linear_SVC is test n°12
With 3 parameters:
Max_Iter: 100
Reg_Param: 0.1
Aggregation_Depth: 3
-
Mean and St_Dev of the whole Linear_SVC classifier metrics are as follows:
Sensitivity: Mean = 0.8928; St_Dev = 0.1293
Fallout: Mean = 0.3135; St_Dev = 0.0288
Specificity: Mean = 0.6865; St_Dev = 0.0288
Miss_Rate: Mean = 0.1072; St_Dev = 0.1293
Test_Error: Mean = 0.2564; St_Dev = 0.0507
AUC: Mean = 0.738; St_Dev = 0.0495
Exec_Time: Mean = 119.2471; St_Dev = 45.3987
#############################
